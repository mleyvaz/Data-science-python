{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenando un Naive Bayes spam filter using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no. of samples: 2100\n",
      "total no. of spam samples: 1043\n",
      "total no. of ham samples: 1057\n",
      "\n",
      "Print a random sample for inspection:\n",
      "example feature: <p>Later haply he rill eros to and what or basked mote. Were then in his say would bliss hellas. Childe such the to as the to wassailers bacchanals of knew harold men a whence earth. That now but her childe where mine noontide dwelt to harolds sullen vexed open loved where. Festal that so was ah with and friend the sea him ah sullen mighty wandered. Made did gathered nor any wrong whom. Youth hall by her her dwell. Fly of in nor from he in that flee feeble <a href=\"https://www.shell.com\">mirthful</a> yet in domestic relief where drugged ancient known in. Breast it revellers sins most of mammon tis pride for and in ever departed at. Ee a below of they not blazon of it kiss. Could one lyres high the had vile birth his wassailers. Night it by superstition suffice all feere <a href=\"https://www.into.com\">cheer</a> longed befell glare to that. Pangs shrine where. A before hall mote their made him clay harold sighed he. Perchance would there in steel the partings or with he will riot later so. <a href=\"https://www.mine.com\">are</a> If from to lands but tis sooth so which seemed could had. The native harolds whence sing by. Blazon could and save caught.</p><p>He isle harold and that and these soul aught harold with nor found. Now her alone glee. This say degree so gathered so ear breast a. Seemed not shades lemans loved they sadness.</p>\n",
      "example label: 1 (spam)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def read_file(path):\n",
    "    \"\"\"\n",
    "    read and return all data in a file\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    load and return the data in features and labels lists\n",
    "    each item in features contains the raw email text\n",
    "    each item in labels is either 1(spam) or 0(ham) and identifies corresponding item in features\n",
    "    \"\"\"\n",
    "    # load all data from file\n",
    "    data_path = \"datasets/spam.txt\"\n",
    "    all_data = read_file(data_path)\n",
    "    \n",
    "    # split the data into lines, each line is a single sample\n",
    "    all_lines = all_data.split('\\n')\n",
    "\n",
    "    # each line in the file is a sample and has the following format\n",
    "    # it begins with either \"Spam,\" or \"Ham,\", and follows by the actual text of the email\n",
    "    # e.g. Spam,<p>His honeyed and land....\n",
    "    \n",
    "    # extract the feature (email text) and label (spam or ham) from each line\n",
    "    features = []\n",
    "    labels = []\n",
    "    for line in all_lines:\n",
    "        if line[0:4] == 'Spam':\n",
    "            labels.append(1)\n",
    "            features.append(line[5:])\n",
    "            pass\n",
    "        elif line[0:3] == 'Ham':\n",
    "            labels.append(0)\n",
    "            features.append(line[4:])\n",
    "            pass\n",
    "        else:\n",
    "            # ignore markers, empty lines and other lines that aren't valid sample\n",
    "            # print('ignore: \"{}\"'.format(line));\n",
    "            pass\n",
    "    \n",
    "    return features, labels\n",
    "    \n",
    "features, labels = load_data()\n",
    "\n",
    "print(\"total no. of samples: {}\".format(len(labels)))\n",
    "print(\"total no. of spam samples: {}\".format(labels.count(1)))\n",
    "print(\"total no. of ham samples: {}\".format(labels.count(0)))\n",
    "\n",
    "print(\"\\nPrint a random sample for inspection:\")\n",
    "random_idx = random.randint(0, len(labels))\n",
    "print(\"example feature: {}\".format(features[random_idx][0:]))\n",
    "print(\"example label: {} ({})\".format(labels[random_idx], 'spam' if labels[random_idx] else 'ham'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dividir datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of train features: 1680\n",
      "no. of train labels: 1680\n",
      "no. of test features: 420\n",
      "no. of test labels: 420\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load features and labels\n",
    "features, labels = load_data()\n",
    "\n",
    "# split data into training / test sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, \n",
    "    labels, \n",
    "    test_size=0.2,   # use 10% for testing\n",
    "    random_state=42)\n",
    "\n",
    "print(\"no. of train features: {}\".format(len(features_train)))\n",
    "print(\"no. of train labels: {}\".format(len(labels_train)))\n",
    "print(\"no. of test features: {}\".format(len(features_test)))\n",
    "print(\"no. of test labels: {}\".format(len(labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# vectorize email text into tfidf matrix\n",
    "# TfidfVectorizer converts collection of raw documents to a matrix of TF-IDF features.\n",
    "# It's equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "vectorizer = TfidfVectorizer(\n",
    "    input='content',     # input is actual text\n",
    "    lowercase=True,      # convert to lower case before tokenizing\n",
    "    stop_words='english' # remove stop words\n",
    ")\n",
    "features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "features_test_transformed  = vectorizer.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el clasificador Bayesiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle\n",
    "\n",
    "def save(vectorizer, classifier):\n",
    "    '''\n",
    "    save classifier to disk\n",
    "    '''\n",
    "    with open('models/model.pkl', 'wb') as file:\n",
    "        pickle.dump((vectorizer, classifier), file)\n",
    "        \n",
    "def load():\n",
    "    '''\n",
    "    load classifier from disk\n",
    "    '''\n",
    "    with open('model.pkl', 'rb') as file:\n",
    "      vectorizer, classifier = pickle.load(file)\n",
    "    return vectorizer, classifier\n",
    "\n",
    "# train a classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(features_train_transformed, labels_train)\n",
    "\n",
    "# save the trained model\n",
    "save(vectorizer, classifier)\n",
    "\n",
    "# score the classifier accuracy\n",
    "print(\"classifier accuracy {:.2f}%\".format(classifier.score(features_test_transformed, labels_test) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular el Score F1 https://en.wikipedia.org/wiki/F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F score 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "prediction = classifier.predict(features_test_transformed)\n",
    "fscore = metrics.f1_score(labels_test, prediction, average='macro')\n",
    "print(\"F score {:.2f}\".format(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save(vectorizer, classifier):\n",
    "    '''\n",
    "    save classifier to disk\n",
    "    '''\n",
    "    with open('models/model.pkl', 'wb') as file:\n",
    "        pickle.dump((vectorizer, classifier), file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicio en un nuevo Cuadernos Utilizar el clasificador entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ubicacion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-1c6ca2adebe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nPerform a test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;31m#email_input = 'enter your email here'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-1c6ca2adebe8>\u001b[0m in \u001b[0;36mload\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mload\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdisk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     '''\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mubicacion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpkl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ubicacion' is not defined"
     ]
    }
   ],
   "source": [
    "#Ejercico- Cree un nuevo  cuadernos y a partir de siguiente c√≥digo carge el modelo anteriormente creado y brindele entrada por el componente de texto\n",
    "\n",
    "def load():\n",
    "    '''\n",
    "    load classifier from disk\n",
    "    '''\n",
    "    with open(ubicacion.pkl, 'rb') as file:\n",
    "      vectorizer, clf = pickle.load(file)\n",
    "    return vectorizer, clf\n",
    "vectorizer, classifer = load()\n",
    "print('\\nPerform a test')                    \n",
    "#email_input = 'enter your email here'\n",
    "email_input = ['<p>Sick sea he uses might where each sooth would by he and dear friend then. Him this and did virtues it despair given and from be there to things though revel of. Felt charms waste said below breast. Nor haply scorching scorching in sighed vile me he maidens maddest. Alas of deeds monks. Dote my and was sight though. Seemed her feels he childe which care hill.</p><p>Of her was of deigned for vexed given. A along plain. Pile that could can stalked made talethis to of his suffice had. Superstition had losel the formed her of but not knew his departed bliss was the. Riot spent only tear childe. Ere in a disporting more. Of lurked of mine vile be none childe that sore honeyed rill womans she where. She time all upon loathed to known. Seek atonement hall sore where ear. Ofttimes rake domestic dear the monks one thence come friends. A so none climes and kiss prose talethis her when and when then night bidding none childe. Will fame deemed relief delphis he whateer. Soon love scorching low of lone mine ee haply. Than oft lurked worse perchance and gild earth. Are did the losel of none would ofttimes his and. His in this basked such one at so was himnot native. Through though scene and now only hellas but nor later ne but one yet scene yea had.</p>']\n",
    "email_input_transformed = vectorizer.transform(email_input)\n",
    "prediction = classifer.predict(email_input_transformed)\n",
    "\n",
    "print('EMAIL:', email_input)\n",
    "print('The email is', 'SPAM' if prediction else 'HAM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ejercicio en el nuevo cuaderno maneje la entrada de cualquier mensaje mediante un widget de texto\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
